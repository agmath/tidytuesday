---
title: "GPT Detectors (Tidy Tuesday)"
author: "Adam Gilbert"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(kableExtra)
library(patchwork)

detectors <- read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-07-18/detectors.csv")

detectors %>%
  head() %>%
  kable() %>%
  kable_styling(bootstrap_options = c("hover", "striped"))
```

Are AI detectors biased in their ability to identify human generated text when the author is a native English speaker versus a non-native English speaker? How well do these AI detectors fare when presented text generated by GPT-3 versus GPT-4?

```{r fig.alt = "A collection of four side-by-side boxplots comparing the performance of several AI detectors on writing samples from native English speakers, non-native english speakers, GPT-3, and GPT-4. Bias against non-native English speakers is present across all models, as this subset of writing samples has the highest median predicted probability of AI generation out of any of the four groups."}

detectors %>%
  select(native, detector, model, .pred_AI) %>%
  mutate(type = ifelse(model == "Human", native, model)) %>%
  mutate(type = case_when(
    (type == "No") ~ "Non-Native",
    (type == "Yes") ~ "Native",
    TRUE ~ type
  )) %>%
  mutate(type = factor(type, levels = c("Native", "GPT3", "Non-Native", "GPT4"))) %>%
  ggplot() + 
  geom_boxplot(aes(x = .pred_AI,
                   y = detector,
                   fill = detector)) +
  facet_wrap(~ type) +
  labs(title = "AI Detector Performance on Writing Samples", 
       subtitle = "Native and Non-Native English Speakers (left), Machine-generated text (right)",
       x = "Predicted Probability of AI Generation",
       y = "",
       caption = "Caption. Four sets of side-by-side boxplots showing bias of AI detection algorithms against non-native English speakers.") +
  theme(legend.pos = "None")
```

As suggested in this week's `tidytuesday` prompt, we see real flaws in these AI detectors. On average, the detectors are able to identify human generated text authored by native English speakers. The detectors are abysmal when presented human generated text authored by non-native English speakers. The detectors are actually harmful to this population, as we can even see that they assign higher likelihoods, on average, or AI generation to these samples than they do to actual machine-generated text from GPT-4 or even GPT-3.