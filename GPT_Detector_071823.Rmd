---
title: "GPT Detectors (Tidy Tuesday)"
author: "Adam Gilbert"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(kableExtra)
library(patchwork)

detectors <- read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2023/2023-07-18/detectors.csv")

detectors %>%
  head() %>%
  kable() %>%
  kable_styling(bootstrap_options = c("hover", "striped"))
```

Are AI detectors biased in their ability to identify human generated text when the author is a native English speaker versus a non-native English speaker? How well do these AI detectors fare when presented text generated by GPT-3 versus GPT-4?

```{r fig.alt = "A collection of four plots comparing the performance of several AI detectors on writing samples from native English spearkers, non-native english speakers, GPT-3, and GPT-4. Bias against non-native English speakers is present across all models, as this population of writing samples has the highest median predicted probability of AI generation out of any of the four groups."}
p1 <- detectors %>%
  filter(model == "Human") %>%
  mutate(native = ifelse(native == "No", "Non-Native", "Native")) %>%
  ggplot() +
  #geom_density(aes(x = .pred_AI, fill = detector)) + 
  geom_boxplot(aes(x = .pred_AI, y = detector, fill = detector)) + 
  #scale_x_log10() +
  facet_wrap(~native, ncol = 1) + 
  labs(title = "AI Detectors on Human Text",
       subtitle = "Native and Non-Native English Speakers") +
  theme(legend.pos = "None")

p2 <- detectors %>%
  filter(model != "Human") %>%
  ggplot() +
  #geom_density(aes(x = .pred_AI, fill = detector)) + 
  geom_boxplot(aes(x = .pred_AI, y = detector, fill = detector)) + 
  #scale_x_log10() +
  facet_wrap(~model, ncol = 1) + 
  labs(title = "AI Detectors on Generated Text",
       subtitle = "by Generating Model",
       x = "Predicted Probability of AI") +
  theme(legend.pos = "None")

p1 + p2
```

As suggested in this week's `tidytuesday` prompt, we see real flaws in these AI detectors. On average, the detectors are able to identify human generated text authored by native English speakers. The detectors are abysmal when presented human generated text authored by non-native English speakers. The detectors are actually harmful to this population, as we can even see that they assign higher likelihoods, on average, or AI generation to these samples than they do to actual machine-generated text from GPT-4 or even GPT-3.